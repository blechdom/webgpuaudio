"use strict";(self.webpackChunkwebgpuaudio=self.webpackChunkwebgpuaudio||[]).push([[844],{4916:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>h,default:()=>S,frontMatter:()=>d,metadata:()=>l,toc:()=>m});var i=t(5893),s=t(1151),o=t(7294),a=t(2308),r=t(1077),u=t(8281);class f{workgroupSize=0;constructor(e){this.audioContext=new AudioContext,this.sampleRate=this.audioContext.sampleRate,this.chunkDurationInSeconds=e,this.chunkNumSamplesPerChannel=this.sampleRate*e,this.chunkNumSamples=2*this.chunkNumSamplesPerChannel,this.chunkBufferSize=this.chunkNumSamples*Float32Array.BYTES_PER_ELEMENT}async initGPU(e){let{code:n,entryPoint:t,workgroupSize:i}=e;this.workgroupSize=i;const s=await navigator.gpu.requestAdapter();this.device=await s.requestDevice(),this.timeInfoBuffer=this.device.createBuffer({size:Float32Array.BYTES_PER_ELEMENT,usage:GPUBufferUsage.UNIFORM|GPUBufferUsage.COPY_DST}),this.chunkBuffer=this.device.createBuffer({size:this.chunkBufferSize,usage:GPUBufferUsage.STORAGE|GPUBufferUsage.COPY_SRC}),this.chunkMapBuffer=this.device.createBuffer({size:this.chunkBufferSize,usage:GPUBufferUsage.MAP_READ|GPUBufferUsage.COPY_DST}),this.audioParamBuffer=this.device.createBuffer({size:3*Float32Array.BYTES_PER_ELEMENT,usage:GPUBufferUsage.STORAGE|GPUBufferUsage.COPY_DST}),this.audioShaderModule=this.device.createShaderModule({code:n}),this.pipeline=this.device.createComputePipeline({layout:"auto",compute:{module:this.audioShaderModule,entryPoint:t,constants:{SAMPLE_RATE:this.sampleRate,WORKGROUP_SIZE:i}}}),this.bindGroup=this.device.createBindGroup({layout:this.pipeline.getBindGroupLayout(0),entries:[{binding:0,resource:{buffer:this.timeInfoBuffer}},{binding:1,resource:{buffer:this.chunkBuffer}},{binding:2,resource:{buffer:this.audioParamBuffer}}]})}playSound(e,n,t){console.log("playSound with sound type ",t),(async()=>{await this.createSoundChunk(e,n,t)})()}async createSoundChunk(e,n,t){if(!this.audioContext)return;this.device.queue.writeBuffer(this.timeInfoBuffer,0,new Float32Array([0])),console.log("create with soundType ",t),this.device.queue.writeBuffer(this.audioParamBuffer,0,new Float32Array([e,n,t]));const i=this.device.createCommandEncoder(),s=i.beginComputePass();s.setPipeline(this.pipeline),s.setBindGroup(0,this.bindGroup),s.dispatchWorkgroups(Math.ceil(this.chunkNumSamplesPerChannel/this.workgroupSize)),s.end(),i.copyBufferToBuffer(this.chunkBuffer,0,this.chunkMapBuffer,0,this.chunkBufferSize),this.device.queue.submit([i.finish()]),await this.chunkMapBuffer.mapAsync(GPUMapMode.READ,0,this.chunkBufferSize);const o=new Float32Array(this.chunkNumSamples);o.set(new Float32Array(this.chunkMapBuffer.getMappedRange())),this.chunkMapBuffer.unmap();const a=this.audioContext.createBuffer(2,this.chunkNumSamplesPerChannel,this.audioContext.sampleRate),r=[];for(let f=0;f<2;++f)r.push(a.getChannelData(f));for(let f=0;f<a.length;++f)for(const[e,n]of r.entries())n[f]=o[2*f+e];const u=this.audioContext.createBufferSource();u.buffer=a,u.connect(this.audioContext.destination),u.start(0)}updateAudioParams(e,n,t){console.log("update with soundType ",t),this.device.queue.writeBuffer(this.audioParamBuffer,0,new Float32Array([e,n,t]))}async stop(){this.audioContext&&await this.audioContext.suspend(),this.audioContext&&await this.audioContext.close()}}var c=t(2402);const d={title:"WGSL Audio Editor, One-Shots",sidebar_position:3},h=void 0,l={id:"wgslEditor/WgslAudioOneShots",title:"WGSL Audio Editor, One-Shots",description:"const [playing, setPlaying] = useState(false);",source:"@site/docs/wgslEditor/WgslAudioOneShots.mdx",sourceDirName:"wgslEditor",slug:"/wgslEditor/WgslAudioOneShots",permalink:"/webgpuaudio/docs/wgslEditor/WgslAudioOneShots",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:3,frontMatter:{title:"WGSL Audio Editor, One-Shots",sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"WGSL Audio Editor, with real-time inputs",permalink:"/webgpuaudio/docs/wgslEditor/WgslAudioEditorWithInputs"},next:{title:"Acid Synth",permalink:"/webgpuaudio/docs/wgslEditor/AcidSynth"}},p={},m=[],g=function(){const e={li:"li",ul:"ul",...(0,s.a)()},n=["guitar","kick","snare","squelch"],t=[1,2,4,8,16,32,64,128,256],[d,h]=(0,o.useState)(!1),[l,p]=(0,o.useState)(void 0),[m,g]=o.useState(c.Z),[v,S]=(0,o.useState)(440),[w,k]=(0,o.useState)(0);(0,o.useEffect)((()=>()=>{B()}),[]);const y=(0,o.useCallback)(((e,n)=>{console.log("val:",e),g(e)}),[]),{chunkDurationInSeconds:P,workgroupSize:A,loadShader:b,timestep:_,volume:x,soundType:E}=(0,u.M4)({soundType:{options:n},frequency:{value:60,min:0,max:100,step:1e-4,onChange:e=>{const n=Math.log(40),t=(Math.log(3e3)-n)/100;S(Math.exp(n+t*(e-0)))}},string:{value:"Hz",label:v.toFixed(3),editable:!1},volume:{value:.15,min:0,max:1,step:.01},timestep:{value:40,min:1,max:100,step:1},WebGPUSettings:(0,u.so)({loadShader:{options:["oneShot"]},chunkDurationInSeconds:{value:1,min:.03,max:2,step:.01},workgroupSize:{options:t,value:t[8]}},{collapsed:!0}),[d?"Stop Sound":"Play Sound"]:(0,u.LI)((()=>{h(!d)}))},[d,v]);async function B(){l&&(await l.stop(),k(0),p(void 0))}return(0,o.useEffect)((()=>{if(d){let e;!async function(){if(void 0===l){const e=new f(P);await e.initGPU({code:m,entryPoint:"synthesize",workgroupSize:A}),p(e)}}();const n=()=>{k((e=>e+1)),e=requestAnimationFrame(n)};return e=requestAnimationFrame(n),()=>cancelAnimationFrame(e)}B()}),[d]),(0,o.useEffect)((()=>{w%_==0&&l&&l.playSound(v,x,n.indexOf(E))}),[l,w,_,v,x,E]),(0,o.useEffect)((()=>{d&&h(!1)}),[P,A,m]),(0,o.useEffect)((()=>{console.log("shader",b),g(c.Z)}),[b]),(0,o.useEffect)((()=>{l&&l.updateAudioParams&&l.updateAudioParams(v,x,n.indexOf(E))}),[l,v,x,E]),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(e.ul,{children:[(0,i.jsx)(e.li,{children:"Audio Synthesis uses rough streaming architecture to get chunks out of WebGPU and send control buffers to control a WebGPU compute shader."}),(0,i.jsx)(e.li,{children:"Select different shaders in the Leva control panel to hear different sounds."}),(0,i.jsx)(e.li,{children:"When changing WebGPU Parameters, the sound will stop and will need to be restarted."}),(0,i.jsx)(e.li,{children:"Changing sound parameters will happen at the rate of the chunk duration, so if you change the frequency, it will take a chunk duration to hear the change."}),(0,i.jsx)(e.li,{children:"Fully refresh the page if things get strange."}),(0,i.jsx)(e.li,{children:"The code below in the live wgsl editor creates the audio data in the WebGPU compute shader."})]}),(0,i.jsx)(u.Zf,{flat:!0,oneLineLabels:!0}),(0,i.jsx)(a.ZP,{value:m,width:"90%",height:"400px",extensions:[(0,r.i)()],onChange:y})]})};function v(e){return(0,i.jsx)(i.Fragment,{})}function S(e={}){return(0,i.jsx)(g,{...e,children:(0,i.jsx)(v,{...e})})}},2402:(e,n,t)=>{t.d(n,{Z:()=>i});const i="override WORKGROUP_SIZE: u32 = 256;\noverride SAMPLE_RATE: f32 = 48000.0;\n\nstruct TimeInfo {\n    offset: f32,\n}\n\nstruct AudioParam {\n  frequency: f32,\n  gain: f32,\n  oneShot: f32,\n}\n\n@binding(0) @group(0) var<uniform> time_info: TimeInfo;\n@binding(1) @group(0) var<storage, read_write> sound_chunk: array<vec2<f32>>; // 2 channel pcm data\n@binding(2) @group(0) var<storage, read> audio_param: AudioParam;\n\n@compute\n@workgroup_size(WORKGROUP_SIZE)\nfn synthesize(@builtin(global_invocation_id) global_id: vec3<u32>) {\n    let sampleCount: u32 = global_id.x;\n\n    if (sampleCount >= arrayLength(&sound_chunk)) {\n        return;\n    }\n    let t = f32(sampleCount) / SAMPLE_RATE;\n\n    if (audio_param.oneShot == 0.0){\n        sound_chunk[sampleCount] = guitar(time_info.offset + t, audio_param.frequency, audio_param.gain);\n    } else if (audio_param.oneShot == 1.0) {\n        sound_chunk[sampleCount] = kick(time_info.offset + t, audio_param.frequency, audio_param.gain);\n  } else if (audio_param.oneShot == 2.0) {\n        sound_chunk[sampleCount] = snare(time_info.offset + t, audio_param.frequency, audio_param.gain);\n    } else {\n        sound_chunk[sampleCount] = guitar(time_info.offset + t, audio_param.frequency, audio_param.gain);\n    }\n}\nconst TAU: f32 = 6.283185307179586476925286766559;\n\nfn window(a: f32, b: f32, t: f32) -> f32\n{\n    return smoothstep(a, (a+b)*0.5, t) * smoothstep(b, (a+b)*0.5, t);\n}\n\n\nfn guitar(time: f32, freq: f32, gain: f32) -> vec2<f32> {\n    const PI: f32 = 3.141592654;\n\n    const L: f32 = 0.635;\n    const h: f32 = 0.125;\n    const d: f32 = 0.15;\n    const GAMMA: f32 = 2.5;\n    const b: f32 = 0.008;\n    const MAX_HARMONICS: u32 = 50u;\n\n    var sig = 0.0;\n\n    for (var n = 0u; n < MAX_HARMONICS; n += 1) {\n        let a_n: f32 = ((2. * h * L * L) / (PI * PI * d * (L - d) * f32(n+1u) * f32(n+1u))) * sin((f32(n+1u) * PI * d) / L );\n        let f_n = f32(n+1u) * freq * sqrt(1. + b * b * f32(n+1u) * f32(n+1u));\n        sig += a_n * sin(TAU * f_n * time) * exp(-f32(n+1u) * GAMMA * freq/200.0 * time);\n    }\n    return vec2(sig * gain);\n}\n\nfn kick(time: f32, freq: f32, gain: f32) -> vec2<f32> {\n    var env: f32 = pow(max(0., 1. - 0.6*time), 1.8);\n    return vec2(env * sin(60.0*time + env*10.0*time));\n}\n\nfn snare(t: f32, freq: f32, gain: f32) -> vec2<f32> {\n\n       // Basic noise-based snare\n       var noi: f32 = coloredNoise(t, 4000., 1000.) + coloredNoise(t, 4000., 3800.) + coloredNoise(t,8000.,7500.) * 0.5;\n       var env: f32 = smoothstep(0.,0.001,t) * smoothstep(0.2,0.05,t);\n       env *= (1. + smoothstep(0.02,0.0,t)); // increase transient\n       env *= (1. - 0.5*window(0.02,0.1,t)); // fake compression\n       var sig: vec2<f32> = vec2(noi) * env;\n       sig = sig/(1.+abs(sig));\n       return vec2(sig * 0.1);\n}\n\nfn noise(s: f32) -> f32 {\n    // Noise is sampled at every integer s\n    // If s = t*f, the resulting signal is close to a white noise\n    // with a sharp cutoff at frequency f.\n\n    // For some reason float(int(x)+1) is sometimes not the same as floor(x)+1.,\n    // and the former produces fewer artifacts?\n    var si: u32 = u32(floor(s));\n    var sf: f32 = fract(s);\n    sf = sf*sf*(3.-2.*sf); // smoothstep(0,1,sf)\n    //sf = sf*sf*sf*(sf*(sf*6.0-15.0)+10.0); // quintic curve\n    // see https://iquilezles.org/articles/texture\n    return mix(rand(f32(si)), rand(f32(si+1)), sf) * 2. - 1.;\n}\n\nfn noise2(s: f32) -> vec2<f32> {\n    var si: u32 = u32(floor(s));\n    var sf: f32 = fract(s);\n    sf = sf*sf*(3.-2.*sf); // smoothstep(0,1,sf)\n    return mix(rand2(f32(si)), rand2(f32(si+1)), sf) * 2. - 1.;\n}\n\n\nfn coloredNoise(t: f32, fc: f32, df: f32) -> f32\n{\n    return sin(TAU*fc*fract(t))*noise(t*df);\n}\n\nfn coloredNoise2(t: f32, fc: f32, df: f32) -> vec2<f32> {\n    // Noise peak centered around frequency fc\n    // containing frequencies between fc-df and fc+df\n    var noiz: vec2<f32> = noise2(t*df);\n    var modul: vec2<f32> = vec2(cos(TAU*fc*t), sin(TAU*fc*t));\n    return modul*noiz;\n}\n\nfn rand(p: f32) -> f32\n{\n    // Hash function by Dave Hoskins\n    // https://www.shadertoy.com/view/4djSRW\n    var q = fract(p * .1031);\n    q *= q + 33.33;\n    q *= q + q;\n    return fract(q);\n}\n\nfn rand2(p: f32) -> vec2<f32>\n{\n    // Hash function by Dave Hoskins\n    // https://www.shadertoy.com/view/4djSRW\n\tvar p3: vec3<f32> = fract(vec3(p) * vec3(.1031, .1030, .0973));\n\tp3 += dot(p3, p3.yzx + 33.33);\n    return fract((p3.xx+p3.yz)*p3.zy);\n}"}}]);