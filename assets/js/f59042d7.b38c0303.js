"use strict";(self.webpackChunkwebgpuaudio=self.webpackChunkwebgpuaudio||[]).push([[0],{6889:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>m,contentTitle:()=>l,default:()=>S,frontMatter:()=>d,metadata:()=>f,toc:()=>p});var i=n(5893),a=n(1151),s=n(7294),o=n(1229),r=n(1077),u=n(8281);class c{timeoutId=null;nextChunkOffset=0;workgroupSize=0;constructor(e){this.audioContext=new AudioContext,this.sampleRate=this.audioContext.sampleRate,this.chunkDurationInSeconds=e,this.chunkNumSamplesPerChannel=this.sampleRate*e,this.chunkNumSamples=2*this.chunkNumSamplesPerChannel,this.chunkBufferSize=this.chunkNumSamples*Float32Array.BYTES_PER_ELEMENT}async initGPU(e){let{code:t,entryPoint:n,workgroupSize:i}=e;this.workgroupSize=i,navigator.gpu||alert("Something went wrong. WebGPU does not appear to be supported. Enable chrome://flags/#enable-webgpu flag.");const a=await navigator.gpu.requestAdapter();this.device=await a.requestDevice(),this.timeInfoBuffer=this.device.createBuffer({size:Float32Array.BYTES_PER_ELEMENT,usage:GPUBufferUsage.UNIFORM|GPUBufferUsage.COPY_DST}),this.chunkBuffer=this.device.createBuffer({size:this.chunkBufferSize,usage:GPUBufferUsage.STORAGE|GPUBufferUsage.COPY_SRC}),this.chunkMapBuffer=this.device.createBuffer({size:this.chunkBufferSize,usage:GPUBufferUsage.MAP_READ|GPUBufferUsage.COPY_DST}),this.audioParamBuffer=this.device.createBuffer({size:16*Float32Array.BYTES_PER_ELEMENT,usage:GPUBufferUsage.STORAGE|GPUBufferUsage.COPY_DST}),this.audioShaderModule=this.device.createShaderModule({code:t}),this.pipeline=this.device.createComputePipeline({layout:"auto",compute:{module:this.audioShaderModule,entryPoint:n,constants:{SAMPLE_RATE:this.sampleRate,WORKGROUP_SIZE:i}}}),this.bindGroup=this.device.createBindGroup({layout:this.pipeline.getBindGroupLayout(0),entries:[{binding:0,resource:{buffer:this.timeInfoBuffer}},{binding:1,resource:{buffer:this.chunkBuffer}},{binding:2,resource:{buffer:this.audioParamBuffer}}]})}playSound(){(async()=>{await this.createSoundChunk()})()}async createSoundChunk(){if(!this.audioContext)return;void 0===this.startTime&&(this.startTime=performance.now()/1e3);const e=this.startTime+this.nextChunkOffset-performance.now()/1e3;if(Math.floor(e/this.chunkDurationInSeconds)>2){const e=this.chunkDurationInSeconds;return void(this.timeoutId=setTimeout(await this.createSoundChunk.bind(this),1e3*e))}this.device.queue.writeBuffer(this.timeInfoBuffer,0,new Float32Array([this.nextChunkOffset]));const t=this.device.createCommandEncoder(),n=t.beginComputePass();n.setPipeline(this.pipeline),n.setBindGroup(0,this.bindGroup),n.dispatchWorkgroups(Math.ceil(this.chunkNumSamplesPerChannel/this.workgroupSize)),n.end(),t.copyBufferToBuffer(this.chunkBuffer,0,this.chunkMapBuffer,0,this.chunkBufferSize),this.device.queue.submit([t.finish()]),await this.chunkMapBuffer.mapAsync(GPUMapMode.READ,0,this.chunkBufferSize);const i=new Float32Array(this.chunkNumSamples);i.set(new Float32Array(this.chunkMapBuffer.getMappedRange())),this.chunkMapBuffer.unmap();const a=this.audioContext.createBuffer(2,this.chunkNumSamplesPerChannel,this.audioContext.sampleRate),s=[];for(let r=0;r<2;++r)s.push(a.getChannelData(r));for(let r=0;r<a.length;++r)for(const[e,t]of s.entries())t[r]=i[2*r+e];const o=this.audioContext.createBufferSource();o.buffer=a,o.connect(this.audioContext.destination),0!==this.nextChunkOffset&&o.start(this.nextChunkOffset),this.nextChunkOffset+=o.buffer.duration,await this.createSoundChunk()}updateAudioParams(e){this.device.queue.writeBuffer(this.audioParamBuffer,0,new Float32Array(e))}async stop(){this.timeoutId&&clearTimeout(this.timeoutId),this.audioContext&&await this.audioContext.suspend(),this.audioContext&&await this.audioContext.close()}}const h="const PARTIALS: u32 = 256u;\nconst PI2: f32 = 6.283185307179586476925286766559;\n\noverride WORKGROUP_SIZE: u32 = 256;\noverride SAMPLE_RATE: f32 = 44100.0;\n\nstruct TimeInfo { offset: f32 }\nstruct AudioParam {\n    bouncePitch: f32,\n    fractalIterations: f32,\n    noiseTime: f32,\n    squelchMod: f32,\n    gain: f32,\n    squelch: f32,\n    squelchSpeed: f32,\n    bounceTime: f32,\n    fractalGain: f32,\n    bounceGain: f32,\n    squelchGain: f32,\n    noiseGain: f32,\n    alarmTime: f32,\n    alarmNoise: f32,\n    alarmGain: f32,\n    }\n\n@group(0) @binding(0) var<uniform> time_info: TimeInfo;\n@group(0) @binding(1) var<storage, read_write> sound_chunk: array<vec2<f32>>; // 2 channel pcm data\n@binding(2) @group(0) var<storage, read> audio_param: AudioParam;\n\n@compute\n@workgroup_size(WORKGROUP_SIZE)\nfn synthesize(@builtin(global_invocation_id) global_id: vec3<u32>) {\n    let sampleCount: u32 = global_id.x;\n\n    if (sampleCount >= arrayLength(&sound_chunk)) {\n        return;\n    }\n\n    let t = f32(sampleCount) / SAMPLE_RATE;\n\n    sound_chunk[sampleCount] = mainSound(time_info.offset + t, audio_param);\n}\n\nfn rand(co: vec2<f32>) -> f32 {\n\treturn fract(sin(dot(co.xy ,vec2(12.9898,78.233))) * 43758.5453);\n}\n\nfn mainSound( time: f32, audio_param: AudioParam ) -> vec2<f32> {\n\n\tvar ti: f32 = time;\n    var s: vec2<f32> = vec2(0.0);\n    var t: f32 = (time%audio_param.squelchSpeed)-(audio_param.squelchSpeed*.5);\n    var tb: f32 = time%audio_param.bounceTime;\n    var tn: f32 = time%audio_param.alarmTime;\n\tvar x: f32 = t*.1;\n\t//fractal sound\n\tvar maxIter: i32 = i32(audio_param.fractalIterations);\n    for (var i=0;i<maxIter;i++) {\n    \tx=1.3/abs(x)-1.;\n    }\n    s+=x*audio_param.fractalGain;\n\t//noise\n\tti*=5.;\n    s+=vec2(rand(vec2(ti,1234.258-ti*2.568))*sin(time*audio_param.noiseTime),0)*audio_param.noiseGain;\n\tti*=2.;\n    s+=vec2(0.,rand(vec2(ti,1234.258-time*2.568))*sin(time*audio_param.noiseTime))*audio_param.noiseGain;\n\tti*=.02;\n    //pulse\n\ts*=.2;\n\ttb-=x*.0003;\n    s+=vec2(sin(time),cos(time))*sin(tb*tb*audio_param.bouncePitch)*exp(-15.*tb)*audio_param.bounceGain;\n\ts+=(1.-((time*(audio_param.squelch+sin(t*t)*audio_param.squelchMod)+x*.2)%2.))*audio_param.squelchGain;\n    //more noise\n\ts*=clamp(time*.2,0.,1.);\n    s+=vec2(cos(tn),sin(tn))*(rand(vec2(tn*1.5,audio_param.alarmNoise*tn))-.3)*audio_param.alarmGain;\n    return vec2(s*audio_param.gain)*clamp(60.-time,0.,1.);\n}",d={title:"Inversion Machine",sidebar_position:5},l=void 0,f={id:"wgslEditor/InversionMachine",title:"Inversion Machine",description:"const [playing, setPlaying] = useState(false);",source:"@site/docs/wgslEditor/InversionMachine.mdx",sourceDirName:"wgslEditor",slug:"/wgslEditor/InversionMachine",permalink:"/docs/wgslEditor/InversionMachine",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:5,frontMatter:{title:"Inversion Machine",sidebar_position:5},sidebar:"tutorialSidebar",previous:{title:"Acid Synth",permalink:"/docs/wgslEditor/AcidSynth"},next:{title:"Web Workers and ring buffers",permalink:"/docs/category/web-workers-and-ring-buffers"}},m={},p=[],g=function(){const e={a:"a",b:"b",i:"i",li:"li",ul:"ul",...(0,a.a)()},t=[1,2,4,8,16,32,64,128,256],[n,d]=(0,s.useState)(!1),[l,f]=(0,s.useState)(),[m,p]=s.useState(h);(0,s.useEffect)((()=>()=>R()),[]);const g=(0,s.useCallback)(((e,t)=>{console.log("val:",e),p(e)}),[]),{volume:v,bouncePitch:S,fractalIterations:b,noiseTime:x,squelchMod:k,squelch:w,squelchSpeed:P,bounceTime:_,chunkDurationInSeconds:G,workgroupSize:y,loadShader:B,fractalGain:C,bounceGain:E,squelchGain:T,noiseGain:M,alarmTime:I,alarmNoise:A,alarmGain:U}=(0,u.M4)({fractalIterations:{value:23,min:18,max:44,step:1},fractalGain:{value:2,min:0,max:4,step:.001},bouncePitch:{value:4e3,min:1,max:2e4,step:1},bounceTime:{value:.3,min:.001,max:4,step:.001},bounceGain:{value:20,min:0,max:40,step:.001},squelch:{value:2e3,min:0,max:5e3,step:.001},squelchSpeed:{value:10,min:.5,max:100,step:.001},squelchMod:{value:100,min:0,max:2e3,step:.001},squelchGain:{value:.7,min:0,max:2,step:.001},noiseTime:{value:10,min:0,max:100,step:.001},noiseGain:{value:4,min:0,max:20,step:.001},alarmTime:{value:4,min:1,max:20,step:.001},alarmNoise:{value:5,min:-6e3,max:6e3,step:.001},alarmGain:{value:2,min:0,max:15,step:.001},volume:{value:.15,min:0,max:1,step:.001},WebGPUSettings:(0,u.so)({loadShader:{options:["inversionMachine"]},chunkDurationInSeconds:{value:.1,min:.03,max:1,step:.01},workgroupSize:{options:t,value:t[8]}},{collapsed:!0}),[n?"Stop Sound":"Play Sound"]:(0,u.LI)((()=>{d(!n)}))},[n]);async function R(){l&&(await l.stop(),f(void 0))}return(0,s.useEffect)((()=>{n?async function(){if(void 0===l){const e=new c(G);await e.initGPU({code:m,entryPoint:"synthesize",workgroupSize:y}),f(e)}}():R()}),[n]),(0,s.useEffect)((()=>{n&&d(!1)}),[G,y,m]),(0,s.useEffect)((()=>{console.log("shader",B),p(h)}),[B]),(0,s.useEffect)((()=>{l&&l.playSound()}),[l]),(0,s.useEffect)((()=>{l&&l.updateAudioParams&&l.updateAudioParams(new Float32Array([S,b,x,k,v,w,P,_,C,E,T,M,I,A,U]))}),[l,S,b,x,k,v,w,P,_,C,E,T,M,I,A,U]),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(e.ul,{children:[(0,i.jsx)(e.li,{children:"Audio Synthesis uses rough streaming architecture to get chunks out of WebGPU and send control buffers to control a WebGPU compute shader."}),(0,i.jsx)(e.li,{children:"Select different shaders in the Leva control panel to hear different sounds."}),(0,i.jsx)(e.li,{children:"When changing WebGPU Parameters, the sound will stop and will need to be restarted."}),(0,i.jsx)(e.li,{children:"Changing sound parameters will happen at the rate of the chunk duration, so if you change the frequency, it will take a chunk duration to hear the change. (Interpolation options coming soon.)"}),(0,i.jsx)(e.li,{children:"Fully refresh the page if things get strange."}),(0,i.jsx)(e.li,{children:"The code below in the live wgsl editor creates the audio data in the WebGPU compute shader."}),(0,i.jsxs)(e.li,{children:["Borrowed heavily from ",(0,i.jsx)(e.b,{children:"The Inversion Machine"})," by ",(0,i.jsx)(e.i,{children:"Kali"})," on shadertoy: ",(0,i.jsx)(e.a,{href:"https://www.shadertoy.com/view/4dsGD7",children:"https://www.shadertoy.com/view/4dsGD7"})]})]}),(0,i.jsx)(u.Zf,{flat:!0,oneLineLabels:!0}),(0,i.jsx)(o.ZP,{value:m,width:"90%",height:"400px",extensions:[(0,r.i)()],onChange:g})]})};function v(e){return(0,i.jsx)(i.Fragment,{})}function S(e={}){return(0,i.jsx)(g,{...e,children:(0,i.jsx)(v,{...e})})}}}]);