"use strict";(self.webpackChunkwebgpuaudio=self.webpackChunkwebgpuaudio||[]).push([[370],{3678:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>m,frontMatter:()=>r,metadata:()=>f,toc:()=>l});var i=t(5893),u=t(1151),a=t(8342);const o="export default class WebGpuAudioEngine {\n  public chunkDurationInSeconds: number;\n  public device: GPUDevice | undefined;\n  public chunkNumSamplesPerChannel: number;\n  public chunkNumSamples: number;\n  public chunkBufferSize: number;\n  public timeInfoBuffer: GPUBuffer;\n  public chunkBuffer: GPUBuffer;\n  public chunkMapBuffer: GPUBuffer;\n  public audioParamBuffer: GPUBuffer;\n  public pipeline: GPUComputePipeline;\n  public bindGroup: GPUBindGroup;\n  public audioShaderModule: GPUShaderModule;\n\n  constructor(\n    numChannels: number,\n    sampleRate: number,\n    workgroupSize: number,\n    chunkDurationInSeconds: number,\n    device: GPUDevice,\n    compute: string,\n    entryPoint: string,\n    audioParamsLength?: number)\n  {\n    this.chunkDurationInSeconds = chunkDurationInSeconds;\n    this.device = device;\n    this.chunkNumSamplesPerChannel = sampleRate * chunkDurationInSeconds;\n    this.chunkNumSamples = numChannels * this.chunkNumSamplesPerChannel;\n    this.chunkBufferSize = this.chunkNumSamples * Float32Array.BYTES_PER_ELEMENT;\n    this.timeInfoBuffer = this.device.createBuffer({\n      size: Float32Array.BYTES_PER_ELEMENT,\n      usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST\n    });\n    this.chunkBuffer = this.device.createBuffer({\n      size: this.chunkBufferSize,\n      usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC,\n    });\n    this.chunkMapBuffer = this.device.createBuffer({\n      size: this.chunkBufferSize,\n      usage: GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST,\n    });\n    this.audioParamBuffer = this.device.createBuffer({\n      size: Float32Array.BYTES_PER_ELEMENT * audioParamsLength,\n      usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,\n    });\n\n    this.audioShaderModule = device.createShaderModule({code: compute});\n    this.pipeline = device.createComputePipeline({\n      layout: 'auto',\n      compute: {\n        module: this.audioShaderModule,\n        entryPoint: entryPoint,\n        constants: {\n          SAMPLE_RATE: sampleRate,\n          WORKGROUP_SIZE: workgroupSize\n        }\n      }\n    });\n\n    this.bindGroup = device.createBindGroup({\n      layout: this.pipeline.getBindGroupLayout(0),\n      entries: [\n        {binding: 0, resource: {buffer: this.timeInfoBuffer}},\n        {binding: 1, resource: {buffer: this.chunkBuffer}},\n        {binding: 2, resource: {buffer: this.audioParamBuffer}},\n      ]\n    });\n  }\n}",s='import { useEffect, useState } from "react"\nimport useIsBrowser from \'@docusaurus/useIsBrowser\';\n\nconst useDevice = (): any => {\n    if (!useIsBrowser() || typeof window === undefined) return;\n    const [adapter, setAdapter] = useState<GPUAdapter | undefined>()\n    const [device, setDevice] = useState<GPUDevice | undefined>()\n\n    useEffect(() => {\n        if (navigator.gpu === undefined) return\n        const initWebGPU = async () => {\n\n            const adapter = await navigator.gpu.requestAdapter()\n\n            if (adapter === null) return\n            const device = await adapter.requestDevice()\n\n            setAdapter(adapter)\n            setDevice(device)\n\n            device.lost.then(() => {\n                setDevice(undefined)\n            })\n        }\n        initWebGPU()\n    }, [])\n\n    if (useIsBrowser() && typeof window !== "undefined" && device !== undefined && adapter !== undefined) {\n        return {\n            adapter,\n            device,\n            gpu: navigator.gpu\n        }\n    } else return;\n\n}\n\nexport default useDevice',r={title:"WebGPU Oscillators"},c="WebGPU Oscillators",f={id:"wgslEditorExamples/webGpuOscillators",title:"WebGPU Oscillators",description:"Live Code Example",source:"@site/docs/wgslEditorExamples/webGpuOscillators.md",sourceDirName:"wgslEditorExamples",slug:"/wgslEditorExamples/webGpuOscillators",permalink:"/docs/wgslEditorExamples/webGpuOscillators",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{title:"WebGPU Oscillators"},sidebar:"tutorialSidebar",previous:{title:"WebAudio Oscillators",permalink:"/docs/wgslEditorExamples/webAudioOscillators"},next:{title:"WebGPU Resources",permalink:"/docs/webGpuResources"}},d={},l=[{value:"Live Code Example",id:"live-code-example",level:2},{value:"WebGPU Audio Engine Class",id:"webgpu-audio-engine-class",level:2},{value:"useDevice Hook",id:"usedevice-hook",level:2}];function h(e){const n={code:"code",h1:"h1",h2:"h2",li:"li",pre:"pre",ul:"ul",...(0,u.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"webgpu-oscillators",children:"WebGPU Oscillators"}),"\n",(0,i.jsx)(n.h2,{id:"live-code-example",children:"Live Code Example"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["This example requires ",(0,i.jsx)("a",{href:"chrome://flags/#enable-webgpu-developer-features",children:"chrome://flags/#enable-webgpu-developer-features"})," flag to be enabled"]}),"\n",(0,i.jsxs)(n.li,{children:["For typescript, webGPU types are available by installing: ",(0,i.jsx)(n.code,{children:"npm i @webgpu/types"})," or ",(0,i.jsx)(n.code,{children:"yarn add @webgpu/types"})]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-tsx",metastring:"live",live:!0,children:'import {setDefaultAutoSelectFamily, setDefaultAutoSelectFamilyAttemptTimeout} from "net";\nimport {jettwaveDark} from "prism-react-renderer";\n\nfunction WebGpuOscillators() {\n\n  const compute = `\n    override WORKGROUP_SIZE: u32 = 256;\n    override SAMPLE_RATE: f32 = 48000.0;\n    const PI2: f32 = 6.283185307179586476925286766559;\n \n    struct TimeInfo {\n        offset: f32,\n    }\n    \n    struct AudioParam {\n      frequency: f32,\n      gain: f32,\n      waveForm: f32\n    }\n    \n    var<private> last_frequency: f32 = 200;\n    \n    @binding(0) @group(0) var<uniform> time_info: TimeInfo;\n    @binding(1) @group(0) var<storage, read_write> sound_chunk: array<vec2<f32>>;\n    @binding(2) @group(0) var<storage, read> audio_param: AudioParam;\n    \n    @compute\n    @workgroup_size(WORKGROUP_SIZE)\n    fn synthesize(@builtin(global_invocation_id) global_id: vec3<u32>) {\n        var sampleCount: u32 = global_id.x; // 0 -> keeps on counting and counting and counting\n    \n        if (sampleCount >= arrayLength(&sound_chunk)) {\n            return;\n        }\n    \n        var t = f32(sampleCount) / SAMPLE_RATE;\n        \n        var chunkPosition = f32(sampleCount % (arrayLength(&sound_chunk)/2))/SAMPLE_RATE; \n        \n        var next_frequency = audio_param.frequency * chunkPosition;\n     \n        sound_chunk[sampleCount] = oscillator(time_info.offset + t, next_frequency, audio_param.gain, audio_param.waveForm);\n    }\n    \n    fn oscillator(time: f32, frequency: f32, gain: f32, waveForm: f32) -> vec2<f32> {\n    \n        \n        var v: f32 = sin(time * frequency * PI2);\n       /* if (waveForm == 1) {\n           v = -abs(fract(time * frequency)-.5)*4.0-1.0;\n        } else if (waveForm == 2) {\n            v = step(fract(time * frequency),0.5)*2.0-1.0;\n        } else if (waveForm == 3) {\n            v = 1.0 - 2.0*fract(time * frequency);\n        }*/\n        return vec2(v * gain);\n    }\n`;\n  const waveForms = ["sine", "triangle", "square", "sawtooth"];\n  const numChannels = 2;\n  const workgroupSize = 16;\n  const chunkDurationInSeconds = .1;\n  const maxBufferedChunks = 2;\n  const [audioContext, setAudioContext] = useState<AudioContext | undefined>(undefined);\n  const [playing, setPlaying] = useState(false);\n  const [engine, setEngine] = useState<any>();\n  const [timeoutId, setTimeoutId] = useState<NodeJS.Timeout | null>(null);\n  const [freq, setFreq] = useState(440);\n  const {device} = useDevice();\n\n  const {volume, frequency, waveForm} = useControls({\n    waveForm: {options: waveForms},\n    frequency: {\n      value: 60,\n      min: 0,\n      max: 100,\n      step: 0.0001,\n      onChange: (v) => {\n        const minIn = 0;\n        const maxIn = 100;\n        const minOut = Math.log(40);\n        const maxOut = Math.log(3000);\n        const scale = (maxOut - minOut) / (maxIn - minIn);\n        setFreq(Math.exp(minOut + scale * (v - minIn)));\n      },\n    },\n    string: {value: \'Hz\', label: freq.toFixed(3), editable: false},\n    volume: {\n      value: 0.15,\n      min: 0.0,\n      max: 1.0,\n      step: 0.01\n    },\n    [playing ? "Stop Sound" : "Play Sound"]: button(() => {\n      setPlaying(!playing)\n    }, {order: -2})\n  }, [playing, freq]);\n\n  useEffect(() => {\n    if (playing) {\n      setAudioContext(new AudioContext());\n    } else {\n      stopMakingSound();\n    }\n\n    async function stopMakingSound() {\n      if (audioContext) await audioContext.suspend();\n      if (audioContext) await audioContext.close();\n      if (timeoutId) clearTimeout(timeoutId);\n      setTimeoutId(null);\n      setAudioContext(undefined);\n      setEngine(undefined);\n    }\n  }, [playing]);\n  \n  useEffect(() => {\n    async function startMakingSound() {\n      if (engine === undefined) {\n        setEngine(new WebGpuAudioEngine(\n          2,\n          audioContext.sampleRate,\n          workgroupSize,\n          chunkDurationInSeconds,\n          device,\n          compute,\n          \'synthesize\',\n          3,\n        ));\n        //setStartTime(performance.now() / 1000.0);\n        //setNextChunkOffset(0);\n      }\n    }\n    if(audioContext) {\n      console.log("sample rate: ", audioContext.sampleRate);\n      startMakingSound();\n    }\n  }, [audioContext]);\n\n  useEffect(() => {\n    if (engine) {\n      playSound();\n    }\n\n    async function playSound() {\n      const startTime = performance.now() / 1000.0;\n      let nextChunkOffset = 0.0;\n\n      async function createSongChunk() {\n        const bufferedSeconds = (startTime + nextChunkOffset) - (performance.now() / 1000.0);\n        const numBufferedChunks = Math.floor(bufferedSeconds / chunkDurationInSeconds);\n        if (numBufferedChunks > maxBufferedChunks) {\n          const timeout = chunkDurationInSeconds * 0.9;\n          setTimeoutId(setTimeout(createSongChunk, timeout * 1000.0));\n          return;\n        }\n\n        device.queue.writeBuffer(engine.timeInfoBuffer, 0, new Float32Array([nextChunkOffset]));\n\n        const commandEncoder = device.createCommandEncoder();\n\n        const pass = commandEncoder.beginComputePass();\n        pass.setPipeline(engine.pipeline);\n        pass.setBindGroup(0, engine.bindGroup);\n        pass.dispatchWorkgroups(\n          Math.ceil(engine.chunkNumSamplesPerChannel / workgroupSize)\n        );\n        pass.end();\n\n        commandEncoder.copyBufferToBuffer(engine.chunkBuffer, 0, engine.chunkMapBuffer, 0, engine.chunkBufferSize);\n\n        device.queue.submit([commandEncoder.finish()]);\n\n        await engine.chunkMapBuffer.mapAsync(GPUMapMode.READ, 0, engine.chunkBufferSize);\n\n        const chunkData = new Float32Array(engine.chunkNumSamples);\n        chunkData.set(new Float32Array(engine.chunkMapBuffer.getMappedRange()));\n        engine.chunkMapBuffer.unmap();\n\n        const audioBuffer = audioContext.createBuffer(\n          numChannels,\n          engine.chunkNumSamplesPerChannel,\n          audioContext.sampleRate\n        );\n\n        const channels = [];\n        for (let i = 0; i < numChannels; ++i) {\n          channels.push(audioBuffer.getChannelData(i));\n        }\n\n        for (let i = 0; i < audioBuffer.length; ++i) {\n          for (const [offset, channel] of channels.entries()) {\n            channel[i] = chunkData[i * numChannels + offset];\n          }\n        }\n\n        const audioSource = audioContext.createBufferSource();\n        audioSource.buffer = audioBuffer;\n        audioSource.connect(audioContext.destination);\n\n        audioSource.start(nextChunkOffset);\n\n        nextChunkOffset += audioSource.buffer.duration;\n        if (playing) await createSongChunk();\n      }\n\n      if (playing) createSongChunk();\n    }\n  }, [engine])\n\n  \n\n  useEffect(() => {\n    if (!engine || !device) return;\n    let waveFormNum = waveForms.indexOf(waveForm);\n    device.queue.writeBuffer(engine.audioParamBuffer, 0, new Float32Array([freq, volume, waveFormNum]));\n  }, [device, engine, freq, volume, waveForm]);\n\n\n  return (\n    <Leva flat oneLineLabels/>\n  )\n}\n'})}),"\n",(0,i.jsx)(n.h2,{id:"webgpu-audio-engine-class",children:"WebGPU Audio Engine Class"}),"\n",(0,i.jsx)(a.Z,{language:"ts",children:o}),"\n",(0,i.jsx)(n.h2,{id:"usedevice-hook",children:"useDevice Hook"}),"\n",(0,i.jsx)(a.Z,{language:"ts",children:s})]})}function m(e={}){const{wrapper:n}={...(0,u.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},3684:(e,n,t)=>{t.d(n,{Z:()=>s});var i=t(7294),u=t(8281);var a=t(3947),o=t(5475);const s={React:i,...i,Leva:u.Zf,useControls:u.M4,useDevice:o.Z,button:u.LI,WebAudioOscillator:class{constructor(){this.audioContext=new AudioContext,this.oscillator=this.audioContext.createOscillator(),this.gain=this.audioContext.createGain(),this.oscillator.connect(this.gain),this.gain.gain.value=0,this.gain.connect(this.audioContext.destination),this.oscillator.start()}setVolume(e){this.gain.gain.setTargetAtTime(e,this.audioContext.currentTime,.01)}setFrequency(e){this.oscillator.frequency.setTargetAtTime(e,this.audioContext.currentTime,.01)}setWaveForm(e){this.oscillator.type=e}stop(){this.gain.gain.setTargetAtTime(0,this.audioContext.currentTime,.01),setTimeout((()=>{this.oscillator.stop(),this.oscillator.disconnect(),this.gain.disconnect(),this.audioContext&&this.audioContext.close(),this.audioContext&&(this.audioContext=void 0)}),100)}},WebGpuAudioEngine:a.Z}},3947:(e,n,t)=>{t.d(n,{Z:()=>i});class i{constructor(e,n,t,i,u,a,o,s){this.chunkDurationInSeconds=i,this.device=u,this.chunkNumSamplesPerChannel=n*i,this.chunkNumSamples=e*this.chunkNumSamplesPerChannel,this.chunkBufferSize=this.chunkNumSamples*Float32Array.BYTES_PER_ELEMENT,this.timeInfoBuffer=this.device.createBuffer({size:Float32Array.BYTES_PER_ELEMENT,usage:GPUBufferUsage.UNIFORM|GPUBufferUsage.COPY_DST}),this.chunkBuffer=this.device.createBuffer({size:this.chunkBufferSize,usage:GPUBufferUsage.STORAGE|GPUBufferUsage.COPY_SRC}),this.chunkMapBuffer=this.device.createBuffer({size:this.chunkBufferSize,usage:GPUBufferUsage.MAP_READ|GPUBufferUsage.COPY_DST}),this.audioParamBuffer=this.device.createBuffer({size:Float32Array.BYTES_PER_ELEMENT*s,usage:GPUBufferUsage.STORAGE|GPUBufferUsage.COPY_DST}),this.audioShaderModule=u.createShaderModule({code:a}),this.pipeline=u.createComputePipeline({layout:"auto",compute:{module:this.audioShaderModule,entryPoint:o,constants:{SAMPLE_RATE:n,WORKGROUP_SIZE:t}}}),this.bindGroup=u.createBindGroup({layout:this.pipeline.getBindGroupLayout(0),entries:[{binding:0,resource:{buffer:this.timeInfoBuffer}},{binding:1,resource:{buffer:this.chunkBuffer}},{binding:2,resource:{buffer:this.audioParamBuffer}}]})}}}}]);