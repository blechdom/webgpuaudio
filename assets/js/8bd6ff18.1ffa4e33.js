"use strict";(self.webpackChunkwebgpuaudio=self.webpackChunkwebgpuaudio||[]).push([[867],{3700:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>h,contentTitle:()=>p,default:()=>S,frontMatter:()=>d,metadata:()=>m,toc:()=>g});var i=n(5893),a=n(7294),r=n(2308),o=n(1077),u=n(8281),s=n(3947),f=n(5475);const c="override WORKGROUP_SIZE: u32 = 256;\noverride SAMPLE_RATE: f32 = 48000.0;\nconst PI2: f32 = 6.283185307179586476925286766559;\n\nstruct TimeInfo {\n    offset: f32,\n}\n\nstruct AudioParam {\n  @location(0) @interpolate(linear) frequency: f32,\n  gain: f32,\n  waveForm: f32\n}\n\n@binding(0) @group(0) var<uniform> time_info: TimeInfo;\n@binding(1) @group(0) var<storage, read_write> sound_chunk: array<vec2<f32>>;\n@binding(2) @group(0) var<storage, read> audio_param: AudioParam;\n\n@compute\n@workgroup_size(WORKGROUP_SIZE)\nfn synthesize(@builtin(global_invocation_id) global_id: vec3<u32>) {\n    var sampleCount: u32 = global_id.x;\n\n    if (sampleCount >= arrayLength(&sound_chunk)) {\n        return;\n    }\n\n    var t = f32(sampleCount) / SAMPLE_RATE;\n\n    sound_chunk[sampleCount] = oscillator(time_info.offset + t, audio_param.frequency, audio_param.gain, audio_param.waveForm);\n}\n\nfn oscillator(time: f32, frequency: f32, gain: f32, waveForm: f32) -> vec2<f32> {\n\n    var v: f32 = sin(time * frequency * PI2);\n    if (waveForm == 1) {\n       v = -abs(fract(time * frequency)-0.5)*4.0-1.0;\n    } else if (waveForm == 2) {\n        v = step(fract(time * frequency),0.5)*2.0-1.0;\n    } else if (waveForm == 3) {\n        v = 1.0 - 2.0*fract(time * frequency);\n    }\n    return vec2(v * gain);\n}\n";var l=n(2389);const d={title:"WGSL Audio Editor, with real-time inputs",sidebar_position:2},p=void 0,m={id:"wgslEditorExamples/WgslAudioEditorWithInputs",title:"WGSL Audio Editor, with real-time inputs",description:"const { device } = useDevice() || {};",source:"@site/docs/wgslEditorExamples/WgslAudioEditorWithInputs.mdx",sourceDirName:"wgslEditorExamples",slug:"/wgslEditorExamples/WgslAudioEditorWithInputs",permalink:"/webgpuaudio/docs/wgslEditorExamples/WgslAudioEditorWithInputs",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:2,frontMatter:{title:"WGSL Audio Editor, with real-time inputs",sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"WGSL Audio Editor, no real-time inputs",permalink:"/webgpuaudio/docs/wgslEditorExamples/WgslAudioNoInput"},next:{title:"WebAudio Oscillators",permalink:"/webgpuaudio/docs/wgslEditorExamples/webAudioOscillators"}},h={},g=[],E=function(){if(!(0,l.Z)()||"undefined"==typeof window)return;const e=["sine","triangle","square","sawtooth"],t=[1,2,4,8,16,32,64,128,256],[n,d]=(0,a.useState)(void 0),[p,m]=(0,a.useState)(!1),[h,g]=(0,a.useState)(),[E,v]=(0,a.useState)(null),[S,w]=(0,a.useState)(440),[B,P]=a.useState(c),{device:b}=(0,f.Z)()||{};if(void 0===b)return;const k=(0,a.useCallback)(((e,t)=>{console.log("val:",e),P(e)}),[]),{volume:_,waveForm:A,chunkDurationInSeconds:y,workgroupSize:M,loadShader:x}=(0,u.M4)({waveForm:{options:e},frequency:{value:60,min:0,max:100,step:1e-4,onChange:e=>{const t=Math.log(40),n=(Math.log(3e3)-t)/100;w(Math.exp(t+n*(e-0)))}},string:{value:"Hz",label:S.toFixed(3),editable:!1},volume:{value:.15,min:0,max:1,step:.01},WebGPUSettings:(0,u.so)({loadShader:{options:["oscillators"]},chunkDurationInSeconds:{value:.15,min:.03,max:1,step:.01},workgroupSize:{options:t,value:t[8]}},{collapsed:!0}),[p?"Stop Sound":"Play Sound"]:(0,u.LI)((()=>{m(!p)}),{order:-2})},[p,S]);return(0,a.useEffect)((()=>{p?d(new AudioContext):async function(){n&&await n.suspend();n&&await n.close();E&&clearTimeout(E);v(null),d(void 0),g(void 0)}()}),[p]),(0,a.useEffect)((()=>{p&&m(!1)}),[y,M,B]),(0,a.useEffect)((()=>{n&&(console.log("sample rate: ",n.sampleRate),async function(){void 0===h&&g(new s.Z(2,n.sampleRate,M,y,b,B,"synthesize",3))}())}),[n]),(0,a.useEffect)((()=>{console.log("shader",x),P(c)}),[x]),(0,a.useEffect)((()=>{h&&async function(){const e=performance.now()/1e3;let t=0;async function i(){if(!n||!p)return;const a=e+t-performance.now()/1e3;if(Math.floor(a/y)>2){return void v(setTimeout(i,1e3*(.9*y)))}b.queue.writeBuffer(h.timeInfoBuffer,0,new Float32Array([t]));const r=b.createCommandEncoder(),o=r.beginComputePass();o.setPipeline(h.pipeline),o.setBindGroup(0,h.bindGroup),o.dispatchWorkgroups(Math.ceil(h.chunkNumSamplesPerChannel/M)),o.end(),r.copyBufferToBuffer(h.chunkBuffer,0,h.chunkMapBuffer,0,h.chunkBufferSize),b.queue.submit([r.finish()]),await h.chunkMapBuffer.mapAsync(GPUMapMode.READ,0,h.chunkBufferSize);const u=new Float32Array(h.chunkNumSamples);u.set(new Float32Array(h.chunkMapBuffer.getMappedRange())),h.chunkMapBuffer.unmap();const s=n.createBuffer(2,h.chunkNumSamplesPerChannel,n.sampleRate),f=[];for(let e=0;e<2;++e)f.push(s.getChannelData(e));for(let e=0;e<s.length;++e)for(const[t,n]of f.entries())n[e]=u[2*e+t];const c=n.createBufferSource();c.buffer=s,c.connect(n.destination),c.start(t),t+=c.buffer.duration,p&&await i()}p&&i()}()}),[h]),(0,a.useEffect)((()=>{if(!h||!b)return;let t=e.indexOf(A);b.queue.writeBuffer(h.audioParamBuffer,0,new Float32Array([S,_,t]))}),[b,h,S,_,A]),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(u.Zf,{flat:!0,oneLineLabels:!0}),(0,i.jsx)(r.ZP,{value:B,width:"90%",height:"400px",extensions:[(0,o.i)()],onChange:k})]})};function v(e){return(0,i.jsx)(i.Fragment,{})}function S(e={}){return(0,i.jsx)(E,{...e,children:(0,i.jsx)(v,{...e})})}},3947:(e,t,n)=>{n.d(t,{Z:()=>i});class i{constructor(e,t,n,i,a,r,o,u){this.chunkDurationInSeconds=i,this.device=a,this.chunkNumSamplesPerChannel=t*i,this.chunkNumSamples=e*this.chunkNumSamplesPerChannel,this.chunkBufferSize=this.chunkNumSamples*Float32Array.BYTES_PER_ELEMENT,this.timeInfoBuffer=this.device.createBuffer({size:Float32Array.BYTES_PER_ELEMENT,usage:GPUBufferUsage.UNIFORM|GPUBufferUsage.COPY_DST}),this.chunkBuffer=this.device.createBuffer({size:this.chunkBufferSize,usage:GPUBufferUsage.STORAGE|GPUBufferUsage.COPY_SRC}),this.chunkMapBuffer=this.device.createBuffer({size:this.chunkBufferSize,usage:GPUBufferUsage.MAP_READ|GPUBufferUsage.COPY_DST}),this.audioParamBuffer=this.device.createBuffer({size:Float32Array.BYTES_PER_ELEMENT*u,usage:GPUBufferUsage.STORAGE|GPUBufferUsage.COPY_DST}),this.audioShaderModule=a.createShaderModule({code:r}),this.pipeline=a.createComputePipeline({layout:"auto",compute:{module:this.audioShaderModule,entryPoint:o,constants:{SAMPLE_RATE:t,WORKGROUP_SIZE:n}}}),this.bindGroup=a.createBindGroup({layout:this.pipeline.getBindGroupLayout(0),entries:[{binding:0,resource:{buffer:this.timeInfoBuffer}},{binding:1,resource:{buffer:this.chunkBuffer}},{binding:2,resource:{buffer:this.audioParamBuffer}}]})}}}}]);